{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of the project I have touched my hand at 4 models (ConvNeXt, ViT, Swin, BEiT(?)). They have all been competitive in terms of performance after fine-tuning, but they are shared the same trait: big.\n",
    "\n",
    "Now, \"size matters\" cuts both ways: larger models (tend to) have better performance, but they are always slower and requires more CPU (and GPU/TPU i.e. accelerated hardware) RAM. If you have infinite computing and processing power (e.g., 100+ NVIDIA 80GB H100s lying around), no problem. But if you are deploying your model to something else, or if latency costs you a lot of money, you are in for a big problem. You will want to reduce the size of the model while retaining as much performance as possible.\n",
    "\n",
    "My first deployment on Hugging Face Space is a Swin-Large model. It fits just fine on the space, but each prediction takes ~5.4s to carry out. I want to explore different alternatives, which may have worse performance but offer better latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helpful rule of thumb I follow is: \"People have already done that.\"\n",
    "\n",
    "I always expect that whatever I can think of, people have already thought of, achieved, or come very close to. There are many reasons behind this, but the interesting corollary is that the first thing I do is looking up what people have done.\n",
    "\n",
    "I found [Jeremy Howard's visualization of `timm`'s benchmark](https://www.kaggle.com/code/jhoward/which-image-models-are-best/) and [Daniel Bourke's result with ViT and EfficientNet](https://www.learnpytorch.io/09_pytorch_model_deployment/) (and while I am at it, yes, I am following Bourke's course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeremy's visualization suggested that I should check out:\n",
    "- LeViT\n",
    "- ViT (okay, it was not even there - just my pick)\n",
    "- EVA-02 (who hates teenagers driving robots to save the world?)\n",
    "- Swin\n",
    "- ConvNeXt\n",
    "- BeIT\n",
    "- EfficientNet\n",
    "\n",
    "from `timm`.\n",
    "\n",
    "And I planed to do exactly that in this notebook, with the help of Pytorch-Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want a model that is:\n",
    "- 95%++ accuracy\n",
    "- As low latency as possible, preferably close to FPS24 (the standard one for old movie with synchonized sound...)\n",
    "- As low memory as possible\n",
    "- High F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifications:\n",
    "- Dataset: Food101 (100% data)\n",
    "- Hardware: 2 $\\times$ NVIDIA GeForce RTX 3090 + CUDA 11.7 + PyTorch 2.0.1\n",
    "- Batch size: 64\n",
    "- Epochs: 3\n",
    "- Optimizer: AdamW\n",
    "- Scheduler: OneCycler\n",
    "- Metrics: Accuracy, F1-Score\n",
    "- Tracking: Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import Food101\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "torch.backends.cuda.allow_tf32=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food101DataModule(L.LightningDataModule):\n",
    "    def __init__(self, transform, data_dir: Union[str, Path] = \"data\", batch_size: int = 64) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        Food101(self.data_dir, split='train', download=True)\n",
    "        Food101(self.data_dir, split='test', download=True)\n",
    "\n",
    "    def setup(self, stage: str = 'fit'):\n",
    "        if stage == 'fit':\n",
    "            food101_full = Food101(self.data_dir, split='train', download=True, transform=self.transform)\n",
    "            self.food101_train, self.food101_val = random_split(food101_full, [0.8, 0.2])\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.food101_test = Food101(self.data_dir, split='test', download=True, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.food101_predict = Food101(self.data_dir, split='test', download=True, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.food101_train, batch_size=64)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.food101_val, batch_size=64)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.food101_test, batch_size=64)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.food101_predict, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food101Classifier(L.LightningModule):\n",
    "    def __init__(self, model_name: str, epochs: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = 101\n",
    "        self.epochs = epochs\n",
    "        self.model = timm.create_model(model_name, pretrained=True, num_classes=101)\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=101)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=101)\n",
    "        self.f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=101)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.train_acc(preds, labels)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        self.model.eval()\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.valid_acc(preds, labels)\n",
    "        self.log('val_acc', self.valid_acc, prog_bar=True)\n",
    "        self.f1_metric(preds, labels)\n",
    "        self.log(\"val_f1\", self.f1_metric, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001, fused=True)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.01, steps_per_epoch=947, epochs=self.epochs)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\" : \"step\" }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"levit_128s.fb_dist_in1k\", \"levit_128.fb_dist_in1k\", \"levit_192.fb_dist_in1k\", \"levit_256.fb_dist_in1k\", \"levit_384.fb_dist_in1k\",\n",
    "          \"convnextv2_nano.fcmae_ft_in22k_in1k_384\", \"convnextv2_tiny.fcmae_ft_in22k_in1k_384\", \"convnextv2_base.fcmae_ft_in22k_in1k_384\",\n",
    "          \"convnextv2_large.fcmae_ft_in22k_in1k_384\", \"tf_efficientnetv2_s.in21k_ft_in1k\", \"tf_efficientnetv2_m.in21k_ft_in1k\",\n",
    "          \"tf_efficientnetv2_l.in21k_ft_in1k\", \"tf_efficientnetv2_b3.in21k_ft_in1k\", \"tf_efficientnet_b2.ns_jft_in1k\",\n",
    "          \"beitv2_large_patch16_224.in1k_ft_in22k_in1k\", \"beitv2_base_patch16_224.in1k_ft_in22k_in1k\", \"vit_base_patch14_dinov2.lvd142m\",\n",
    "          \"vit_large_patch14_dinov2.lvd142m\", \"vit_small_patch14_dinov2.lvd142m\", \"vit_large_patch14_clip_336.laion2b_ft_in12k_in1k_inat21\",\n",
    "          \"vit_large_patch14_clip_336.datacompxl_ft_inat21\", \"eva02_large_patch14_clip_336.merged2b_ft_inat21\", \"vit_relpos_medium_patch16_rpn_224.sw_in1k\",\n",
    "          \"swinv2_tiny_window8_256.ms_in1k\", \"swinv2_small_window8_256.ms_in1k\", \"swinv2_base_window8_256.ms_in1k\", \"timm/swinv2_large_window12to16_192to256.ms_in22k_ft_in1k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    logger = TensorBoardLogger(\"runs\", version=1, name=f\"{model}/logs\")\n",
    "    food_model = Food101Classifier(\"hf_hub:timm/\"+model, 3)\n",
    "    compiled_model = torch.compile(food_model)\n",
    "    data_cfg = timm.data.resolve_data_config(food_model.model.pretrained_cfg)\n",
    "    transform = timm.data.create_transform(**data_cfg)\n",
    "    food_data = Food101DataModule(transform)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_acc\", dirpath=\"models\", filename=f\"{model}/checkpoints\")\n",
    "    trainer = L.Trainer(\n",
    "        logger=logger,\n",
    "        accelerator='gpu',\n",
    "        devices=2,\n",
    "        strategy=\"fsdp|ddp\",\n",
    "        precision=\"bf16-mixed\",\n",
    "        accumulate_grad_batches=1,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        max_epochs=3,\n",
    "        fast_dev_run=False,\n",
    "        profiler=\"advanced\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    trainer.fit(compiled_model, food_data)\n",
    "    trainer.test(compiled_model, food_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
