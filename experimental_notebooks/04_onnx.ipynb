{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import lightning as L\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import Food101\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "class Food101DataModule(L.LightningDataModule):\n",
    "    def __init__(self, train_transform, test_transform, data_dir: Union[str, Path] = \"data\", batch_size: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        Food101(self.data_dir, split='train', download=True) # type: ignore\n",
    "        Food101(self.data_dir, split='test', download=True) # type: ignore\n",
    "\n",
    "    def setup(self, stage: str = 'fit'):\n",
    "        if stage == 'fit':\n",
    "            food101_train = Food101(self.data_dir, split='train', download=True, transform=self.train_transform) # type: ignore\n",
    "            food101_test = Food101(self.data_dir, split='train', download=True, transform=self.test_transform) # type: ignore\n",
    "            self.food101_train, _ = random_split(food101_train, [0.8, 0.2], generator=torch.Generator().manual_seed(42)) # type: ignore\n",
    "            _, self.food101_val = random_split(food101_test, [0.8, 0.2], generator=torch.Generator().manual_seed(42)) # type: ignore\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.food101_test = Food101(self.data_dir, split='test', download=True, transform=self.test_transform) # type: ignore\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.food101_predict = Food101(self.data_dir, split='test', download=True, transform=self.test_transform) # type: ignore\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.food101_train, batch_size=self.batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.food101_val, batch_size=self.batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.food101_test, batch_size=self.batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.food101_predict, batch_size=self.batch_size, num_workers=4, pin_memory=True)\n",
    "\n",
    "class Food101Classifier(L.LightningModule):\n",
    "    def __init__(self, model_name: str = \"hf_hub:timm/levit_256.fb_dist_in1k\") -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = 101\n",
    "        self.model = timm.create_model(model_name, pretrained=True, num_classes=101)\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=101)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=101)\n",
    "        self.f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=101)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.train_acc(preds, labels)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        self.model.eval()\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        self.valid_acc(preds, labels)\n",
    "        self.log('val_acc', self.valid_acc, prog_bar=True, sync_dist=True)\n",
    "        self.f1_metric(preds, labels)\n",
    "        self.log(\"val_f1\", self.f1_metric, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        self.model.eval()\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "        self.valid_acc(preds, labels)\n",
    "        self.log('test_acc', self.valid_acc, prog_bar=True, sync_dist=True)\n",
    "        self.f1_metric(preds, labels)\n",
    "        self.log(\"test_f1\", self.f1_metric, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=0.001, foreach=True)\n",
    "\n",
    "class Food101Predictor:\n",
    "    def __init__(self, model_path) -> None:\n",
    "        self.model_path = model_path\n",
    "        if torch.cuda.is_available():\n",
    "            self.model = Food101Classifier.load_from_checkpoint(model_path) # type: ignore\n",
    "        else:\n",
    "            self.model = Food101Classifier.load_from_checkpoint(model_path, map_location='cpu')\n",
    "        self.model.eval()\n",
    "        self.model.freeze()\n",
    "        self.transform = T.Compose([\n",
    "                    T.Resize(256),\n",
    "                    T.CenterCrop(224),\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        with open('labels.txt', 'r') as f:\n",
    "            self.idx_to_label = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "    def predict(self, input_image):\n",
    "        input_tensor = self.transform(input_image)\n",
    "        input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print('CUDA device detected. Switched to CUDA device for faster inference')\n",
    "            input_batch = input_batch.to('cuda')\n",
    "        else:\n",
    "            print('Using CPU for inference. Will be slower')\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output = self.model(input_batch)\n",
    "        \n",
    "        probabilities = self.softmax(output[0])\n",
    "        top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "        # Label:probability\n",
    "        result = {self.idx_to_label[int(idx)]:val.item() for val, idx in zip(top5_prob.cpu(), top5_catid.cpu())}\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/seefood102/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints-v1\"\n",
    "model_path = f'../models/levit_256/{checkpoint}.ckpt'\n",
    "model = Food101Classifier.load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_sample = torch.randn((1, 3, 224, 224))\n",
    "model.to_onnx(filepath,\n",
    "              input_sample,\n",
    "              export_params=True,\n",
    "              input_names = ['input'],    # Input names\n",
    "              output_names = ['output'],  # Output names\n",
    "              dynamic_axes={              # variable length axes\n",
    "                'input' : {0 : 'batch_size'},\n",
    "                'output' : {0 : 'batch_size'},\n",
    "                }\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "checkpoint = \"checkpoints-v1\"\n",
    "filepath = f\"../models/levit_256/onnx/{checkpoint}.onnx\"\n",
    "ort_session = ort.InferenceSession(filepath)\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "image_path = \"../pablo-pacheco-D3Mag4BKqns-unsplash.jpg\"\n",
    "pil_image = Image.open(image_path)\n",
    "transform = T.Compose([\n",
    "                    T.Resize(248),\n",
    "                    T.CenterCrop(224),\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "input_tensor = transform(pil_image)\n",
    "input_batch = input_tensor.unsqueeze(0).numpy()\n",
    "ort_inputs = {input_name: input_batch}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.6666534 , -4.1379743 ,  0.09322017,  1.8908892 ,  0.1664089 ,\n",
       "         -3.098617  , -2.3675237 ,  2.1152372 , -0.16542673, -3.1601365 ,\n",
       "          0.8644059 ,  1.2101617 , -1.9237795 ,  0.3433872 ,  3.5068617 ,\n",
       "         -5.609584  , -7.5836444 , -0.21742302, -0.6731906 ,  3.0013309 ,\n",
       "         -2.306675  , -1.1696708 , -5.6356754 , -2.240593  ,  1.6441283 ,\n",
       "         -1.3571461 ,  4.203167  , -2.5730233 ,  3.0840838 , -2.4108355 ,\n",
       "         -3.2797666 , -4.184952  ,  3.0077586 ,  2.4307203 , -3.5342126 ,\n",
       "          7.146016  , -1.8910706 , -1.6069123 , -4.5603175 , -3.460476  ,\n",
       "         -1.375018  ,  4.9698696 , -0.41188335,  0.66926116, -2.9214377 ,\n",
       "         -7.6591306 ,  5.591901  ,  1.8317235 , -5.308361  ,  8.514215  ,\n",
       "          1.5019705 , -1.327201  ,  7.0852566 ,  1.1551702 , -5.019644  ,\n",
       "          0.62650657, -3.3534102 , -1.9841982 , -4.920026  ,  8.848688  ,\n",
       "          1.9230804 ,  0.87871134,  4.781182  ,  1.3309536 ,  0.03810632,\n",
       "          0.35104328, -0.6571264 , -0.36623037,  1.2292479 , -1.5576811 ,\n",
       "          2.4637148 , -1.66436   , -0.32667297, -0.64424026, -5.4175987 ,\n",
       "         -1.1033698 ,  3.7269733 ,  1.8823264 ,  0.88894737, -2.566006  ,\n",
       "          1.6327603 , -0.5786642 ,  5.0962105 ,  0.48064214, -2.8120232 ,\n",
       "          1.307544  ,  0.21657825,  2.87125   ,  6.8059344 , -4.7523932 ,\n",
       "          4.117715  ,  1.9959518 , -0.36376578, -0.28692144, -3.9535913 ,\n",
       "         -2.6497076 , -3.0695734 ,  5.8942184 ,  4.1812816 , -3.0673912 ,\n",
       "          3.8336782 ]], dtype=float32),\n",
       " tensor([[ 2.6667, -4.1380,  0.0932,  1.8909,  0.1664, -3.0986, -2.3675,  2.1152,\n",
       "          -0.1654, -3.1601,  0.8644,  1.2102, -1.9238,  0.3434,  3.5069, -5.6096,\n",
       "          -7.5836, -0.2174, -0.6732,  3.0013, -2.3067, -1.1697, -5.6357, -2.2406,\n",
       "           1.6441, -1.3571,  4.2032, -2.5730,  3.0841, -2.4108, -3.2798, -4.1850,\n",
       "           3.0078,  2.4307, -3.5342,  7.1460, -1.8911, -1.6069, -4.5603, -3.4605,\n",
       "          -1.3750,  4.9699, -0.4119,  0.6693, -2.9214, -7.6591,  5.5919,  1.8317,\n",
       "          -5.3084,  8.5142,  1.5020, -1.3272,  7.0853,  1.1552, -5.0196,  0.6265,\n",
       "          -3.3534, -1.9842, -4.9200,  8.8487,  1.9231,  0.8787,  4.7812,  1.3310,\n",
       "           0.0381,  0.3510, -0.6571, -0.3662,  1.2292, -1.5577,  2.4637, -1.6644,\n",
       "          -0.3267, -0.6442, -5.4176, -1.1034,  3.7270,  1.8823,  0.8889, -2.5660,\n",
       "           1.6328, -0.5787,  5.0962,  0.4806, -2.8120,  1.3075,  0.2166,  2.8712,\n",
       "           6.8059, -4.7524,  4.1177,  1.9960, -0.3638, -0.2869, -3.9536, -2.6497,\n",
       "          -3.0696,  5.8942,  4.1813, -3.0674,  3.8337]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "ort_outs[0], torch.Tensor(ort_outs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seefood102",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
