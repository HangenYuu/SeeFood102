{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import gc\n",
    "\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import Food101\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food101DataModule(L.LightningDataModule):\n",
    "    def __init__(self, transform, data_dir: Union[str, Path] = \"data\", batch_size: int = 128) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "\n",
    "    def prepare_data(self):\n",
    "        Food101(self.data_dir, split='train', download=True)\n",
    "        Food101(self.data_dir, split='test', download=True)\n",
    "\n",
    "    def setup(self, stage: str = 'fit'):\n",
    "        if stage == 'fit':\n",
    "            food101_full = Food101(self.data_dir, split='train', download=True, transform=self.transform)\n",
    "            self.food101_train, self.food101_val = random_split(food101_full, [0.8, 0.2])\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.food101_test = Food101(self.data_dir, split='test', download=True, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.food101_predict = Food101(self.data_dir, split='test', download=True, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.food101_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.food101_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.food101_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.food101_predict, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Food101Classifier(L.LightningModule):\n",
    "    def __init__(self, model_name: str, epochs: int) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = 101\n",
    "        self.epochs = epochs\n",
    "        self.model = timm.create_model(model_name, pretrained=True, num_classes=101)\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=101)\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=101)\n",
    "        self.f1_metric = torchmetrics.F1Score(task=\"multiclass\", num_classes=101)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.train_acc(preds, labels)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        self.model.eval()\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.valid_acc(preds, labels)\n",
    "        self.log('val_acc', self.valid_acc, prog_bar=True)\n",
    "        self.f1_metric(preds, labels)\n",
    "        self.log(\"val_f1\", self.f1_metric, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        self.model.eval()\n",
    "        outputs = self.forward(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.valid_acc(preds, labels)\n",
    "        self.log('test_acc', self.valid_acc, prog_bar=True)\n",
    "        self.f1_metric(preds, labels)\n",
    "        self.log(\"test_f1\", self.f1_metric, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001, foreach=True)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.01, steps_per_epoch=947, epochs=self.epochs)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\" : \"step\" }\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"levit_128s.fb_dist_in1k\", \"levit_192.fb_dist_in1k\", \"levit_256.fb_dist_in1k\", \"levit_384.fb_dist_in1k\",\n",
    "          \"convnextv2_nano.fcmae_ft_in22k_in1k_384\", \"convnextv2_tiny.fcmae_ft_in22k_in1k_384\", \"convnextv2_base.fcmae_ft_in22k_in1k_384\",\n",
    "          \"convnextv2_large.fcmae_ft_in22k_in1k_384\", \"tf_efficientnetv2_s.in21k_ft_in1k\", \"tf_efficientnetv2_m.in21k_ft_in1k\",\n",
    "          \"tf_efficientnetv2_l.in21k_ft_in1k\", \"tf_efficientnetv2_b3.in21k_ft_in1k\", \"tf_efficientnet_b2.ns_jft_in1k\",\n",
    "          \"beitv2_large_patch16_224.in1k_ft_in22k_in1k\", \"beitv2_base_patch16_224.in1k_ft_in22k_in1k\", \"vit_base_patch14_dinov2.lvd142m\",\n",
    "          \"vit_large_patch14_dinov2.lvd142m\", \"vit_small_patch14_dinov2.lvd142m\", \"vit_large_patch14_clip_336.laion2b_ft_in12k_in1k_inat21\",\n",
    "          \"vit_large_patch14_clip_336.datacompxl_ft_inat21\", \"eva02_large_patch14_clip_336.merged2b_ft_inat21\", \"vit_relpos_medium_patch16_rpn_224.sw_in1k\",\n",
    "          \"swinv2_tiny_window8_256.ms_in1k\", \"swinv2_small_window8_256.ms_in1k\", \"swinv2_base_window8_256.ms_in1k\", \"timm/swinv2_large_window12to16_192to256.ms_in22k_ft_in1k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levit_256.fb_dist_in1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/seefood102/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | LevitDistilled     | 18.0 M\n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | valid_acc | MulticlassAccuracy | 0     \n",
      "3 | f1_metric | MulticlassF1Score  | 0     \n",
      "-------------------------------------------------\n",
      "18.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.0 M    Total params\n",
      "71.886    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a153acfff7c4880b6f9d1e5b829b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/seefood102/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/root/miniconda3/envs/seefood102/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4940a1408df947238f81ec8305925da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/seefood102/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "for model in models[2:3]:\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(model)\n",
    "    logger = TensorBoardLogger(\"runs\", version=1, name=f\"{model}/logs\")\n",
    "    food_model = Food101Classifier(\"hf_hub:timm/\"+model, 3)\n",
    "    data_cfg = timm.data.resolve_data_config(food_model.model.pretrained_cfg)\n",
    "    transform = timm.data.create_transform(**data_cfg)\n",
    "    food_data = Food101DataModule(transform)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"val_acc\", dirpath=\"models\", filename=f\"{model}/checkpoints\")\n",
    "    trainer = L.Trainer(\n",
    "        logger=logger,\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        precision=\"16-mixed\",\n",
    "        accumulate_grad_batches=1,\n",
    "        enable_checkpointing=True,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        max_epochs=3,\n",
    "        fast_dev_run=False,\n",
    "        profiler=\"advanced\",\n",
    "    )\n",
    "    trainer.fit(food_model, food_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seefood102",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
